{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS42SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "J5n6hxVGIUot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install inflect --upgrade\n",
        "#!pip install contractions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import html\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import inflect\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QpPcbYew_ttN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Working with Text Data\n",
        "\n",
        "### Use Python string methods remove irregular whitespace from the following string:"
      ]
    },
    {
      "metadata": {
        "id": "dtotEnsStY5o",
        "colab_type": "code",
        "outputId": "cd9bb7b1-6593-46ae-f5c5-3ee7ca2d034f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
        "print(whitespace_string)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  This is a    string   that has  \n",
            " a lot of  extra \n",
            "   whitespace.   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G9-MkBwasXx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f429c2bd-7420-4447-c92d-bc1857667aac"
      },
      "cell_type": "code",
      "source": [
        " ##### Your Code Here #####\n",
        "import re\n",
        "whitespace_string = whitespace_string.replace('\\n', ' ') # Use to remove carriage returns. \n",
        "whitespace_string = re.sub(' +', ' ', whitespace_string) # Remove excess whitespace. \n",
        "print(whitespace_string)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " This is a string that has a lot of extra whitespace. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vg1-d2aAsXLn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
        "\n",
        "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
        "\n",
        "- Day\n",
        "- Month\n",
        "- Year\n"
      ]
    },
    {
      "metadata": {
        "id": "KWDiN4C9_0sq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "898f6496-c4a3-40a5-e8df-19fa5eb54527"
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####\n",
        "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt\"\n",
        "df = pd.read_csv(url, names = [\"Month\", \"Day\", \"Year\"], sep = \"(?:,|\\s)\\s*\",\n",
        "                 header = None,  engine='python')\n",
        "df = df[[\"Day\",\"Month\",\"Year\"]]\n",
        "df"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>29</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>19</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>26</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>17</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>24</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>31</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>7</td>\n",
              "      <td>June</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>June</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>21</td>\n",
              "      <td>June</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>28</td>\n",
              "      <td>June</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5</td>\n",
              "      <td>July</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>12</td>\n",
              "      <td>July</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>July</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Day  Month  Year\n",
              "0     8  March  2015\n",
              "1    15  March  2015\n",
              "2    22  March  2015\n",
              "3    29  March  2015\n",
              "4     5  April  2015\n",
              "5    12  April  2015\n",
              "6    19  April  2015\n",
              "7    26  April  2015\n",
              "8     3    May  2015\n",
              "9    10    May  2015\n",
              "10   17    May  2015\n",
              "11   24    May  2015\n",
              "12   31    May  2015\n",
              "13    7   June  2015\n",
              "14   14   June  2015\n",
              "15   21   June  2015\n",
              "16   28   June  2015\n",
              "17    5   July  2015\n",
              "18   12   July  2015\n",
              "19   19   July  2015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "s4Q0dgoe_uBW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Bag of Words \n",
        "\n",
        "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
        "\n",
        "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
        "\n",
        " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
        "\n",
        "1) Text should be lowercase.\n",
        "\n",
        "2) Stopwords should be removed.\n",
        "\n",
        "3) Punctuation should be removed.\n",
        "\n",
        "4) Tweets should be tokenized at the word level. \n",
        "\n",
        "(The above don't necessarily need to be completed in that specific order.)\n",
        "\n",
        "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
      ]
    },
    {
      "metadata": {
        "id": "1xzdhyTS_3F9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1c044ffc-2c27-48e1-d748-e0104656d5c4"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\")\n",
        "df.head()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText\n",
              "0          0                       is so sad for my APL frie...\n",
              "1          0                     I missed the New Moon trail...\n",
              "2          1                            omg its already 7:30 :O\n",
              "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
              "4          0           i think mi bf is cheating on me!!!   ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "f49LJGhKE_oJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "d045361c-4e84-4f5e-fb6b-74897789acae"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\")\n",
        "df.head()\n",
        "\n",
        "### Denoise functions, apply to text ### \n",
        "def encode_decode(text):\n",
        "    \"\"\"Utility function to clean text by decoding HTML text.\"\"\"\n",
        "    unescaped = html.unescape(text)\n",
        "    return unescaped\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "  \n",
        "\n",
        "def denoise_text(text):\n",
        "    text = encode_decode(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "df['SentimentText'] = df['SentimentText'].apply(denoise_text)\n",
        "\n",
        "\n",
        "### Normalizing functions, apply to tokenized lists ###\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words or string\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = (unicodedata.normalize('NFKD', word)\n",
        "                    .encode('ascii','ignore').decode('utf-8', 'ignore'))\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "  \n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words or string, also \n",
        "    lowercases every word.\n",
        "    \"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s\\_]|\\_', '', word)\n",
        "        new_word = new_word.lower()\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "  \n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words or string \n",
        "    with textual representation.\n",
        "    \"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words  \n",
        "  \n",
        "  \n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    #words = remove_stopwords(words)\n",
        "    return words\n",
        "  \n",
        "#def remove_stopwords(words):\n",
        "#    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "#    new_words = []\n",
        "#    for word in words:\n",
        "#        if word not in stopwords:\n",
        "#            new_words.append(word)\n",
        "#    return new_words\n",
        "  \n",
        "  \n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "df['SentimentText'] = df['SentimentText'].apply(tokenizer.tokenize)\n",
        "df['SentimentText'] = [normalize(token) for token in df['SentimentText']]\n",
        "\n",
        "\n",
        "cleaned_words = [word for tokens in df['SentimentText'] for word in tokens]\n",
        "cleanedVOCAB = sorted(list(set(cleaned_words)))\n",
        "display(\"%s words total, with a vocabulary size of %s\" % (len(cleaned_words), len(cleanedVOCAB)))\n",
        "\n",
        "display(df.head(20))\n",
        "\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-a6550029ee75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mcleaned_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SentimentText'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mcleanedVOCAB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s words total, with a vocabulary size of %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanedVOCAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Series' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Q764vszGqiUh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How should TF-IDF scores be interpreted? How are they calculated?"
      ]
    },
    {
      "metadata": {
        "id": "e2Ji7BMhqs3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for **Term Frequency - Inverse Document Frequency**\n",
        "\n",
        "**Term Frequency** shows how frequently an expression (term, word) occurs in a document, with the insight being that the more frequently a term is used, the greater its importance.  It is calculated by taking the number of times a word appears in a document, divided by the total number of words in that document\n",
        "\n",
        "In sklearn TF-IDF Transformer it is caluclated by: ***tf(t, d)***\n",
        "\n",
        "It is well understood however the correlation between Term Frequency != Importance in all cases. \n",
        "Often we see terms that occur so often they necessitate removal via stopwords removal, as they are trivial but frequent. Hence frequency alone is not conclusive. \n",
        "\n",
        "**Inverse Document Frequency**  It is a method of determining the frequency of a word within a dataset of texts. Imagine it as a ratio of all existing texts/documents of an entire dataset and the number of texts that contain the defined keyword.  \n",
        "\n",
        "In sklearn TF-IDF Transformer it is caluclated by: ***idf(t) = log [ n / df(t) ] + 1***\n",
        "\n",
        "n = total_#_of_documents\n",
        "df(t) #_of_docs_containing_term;  the document frequency of t;  the number of documents in the document set that contain the term t. \n",
        "\n",
        "As the document Frequency grows this fraction (total/target) becomes smaller.\n",
        "\n",
        "**TF-IDF** or more accurately (*TF* x *IDF*) is where we multiply the Term Frequency by the Inverse Document Frequency.  We essentially weight the Term Frequency by the Inverse Document Frequency so that words that occur too frequently are penalized, and words that may be more infrequent but appear across multiple documents are boosted.  This better helps sharpen the signal of our input data and allows our models to operate with a clearer distinction between signal (important words) & noise (frequent but inconsequential words). \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3iUeBKtG_uEK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Document Classification\n",
        "\n",
        "1) Use Train_Test_Split to create train and test datasets.\n",
        "\n",
        "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
        "\n",
        " - Stretch goal: Use both of the methods that we talked about in class.\n",
        "\n",
        "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
        "\n",
        "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
        "\n",
        " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TX8OEgUP_3ee",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sorF95UO_uGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4 -  Word2Vec\n",
        "\n",
        "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
        "\n",
        "2) Display the 10 words that are most similar to the word \"twitter\""
      ]
    },
    {
      "metadata": {
        "id": "DYno4d4N-LHR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}