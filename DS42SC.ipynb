{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS42SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "J5n6hxVGIUot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "e424baab-184d-4ccd-f128-548d9e20b516"
      },
      "cell_type": "code",
      "source": [
        "!pip install inflect --upgrade\n",
        "!pip install contractions\n",
        "!pip install -U gensim\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import html\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import inflect\n",
        "import contractions\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: inflect in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.17)\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/6c93685bed0026b6a1cce55ab173f6b617f6db0d1325d25489c2fd43e711/gensim-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.2MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.115)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.115 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.115)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QpPcbYew_ttN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Working with Text Data\n",
        "\n",
        "### Use Python string methods remove irregular whitespace from the following string:"
      ]
    },
    {
      "metadata": {
        "id": "dtotEnsStY5o",
        "colab_type": "code",
        "outputId": "18aeefc2-3b44-424d-ce10-a83761e07aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
        "print(whitespace_string)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  This is a    string   that has  \n",
            " a lot of  extra \n",
            "   whitespace.   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G9-MkBwasXx8",
        "colab_type": "code",
        "outputId": "2d8509e2-0d21-471a-bf5b-09652f5f0967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        " ##### Your Code Here #####\n",
        "import re\n",
        "whitespace_string = whitespace_string.replace('\\n', ' ') # Use to remove carriage returns. \n",
        "whitespace_string = re.sub(' +', ' ', whitespace_string) # Remove excess whitespace. \n",
        "print(whitespace_string)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " This is a string that has a lot of extra whitespace. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vg1-d2aAsXLn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
        "\n",
        "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
        "\n",
        "- Day\n",
        "- Month\n",
        "- Year\n"
      ]
    },
    {
      "metadata": {
        "id": "KWDiN4C9_0sq",
        "colab_type": "code",
        "outputId": "a347f5cd-7396-4d5a-aa40-be5b1824b204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####\n",
        "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt\"\n",
        "df = pd.read_csv(url, names = [\"Month\", \"Day\", \"Year\"], sep = \"(?:,|\\s)\\s*\",\n",
        "                 header = None,  engine='python')\n",
        "df = df[[\"Day\",\"Month\",\"Year\"]]\n",
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>29</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>19</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>26</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>17</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>24</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>31</td>\n",
              "      <td>May</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>7</td>\n",
              "      <td>June</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>June</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>21</td>\n",
              "      <td>June</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>28</td>\n",
              "      <td>June</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5</td>\n",
              "      <td>July</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>12</td>\n",
              "      <td>July</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>July</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Day  Month  Year\n",
              "0     8  March  2015\n",
              "1    15  March  2015\n",
              "2    22  March  2015\n",
              "3    29  March  2015\n",
              "4     5  April  2015\n",
              "5    12  April  2015\n",
              "6    19  April  2015\n",
              "7    26  April  2015\n",
              "8     3    May  2015\n",
              "9    10    May  2015\n",
              "10   17    May  2015\n",
              "11   24    May  2015\n",
              "12   31    May  2015\n",
              "13    7   June  2015\n",
              "14   14   June  2015\n",
              "15   21   June  2015\n",
              "16   28   June  2015\n",
              "17    5   July  2015\n",
              "18   12   July  2015\n",
              "19   19   July  2015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "s4Q0dgoe_uBW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Bag of Words \n",
        "\n",
        "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
        "\n",
        "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
        "\n",
        " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
        "\n",
        "1) Text should be lowercase.\n",
        "\n",
        "2) Stopwords should be removed.\n",
        "\n",
        "3) Punctuation should be removed.\n",
        "\n",
        "4) Tweets should be tokenized at the word level. \n",
        "\n",
        "(The above don't necessarily need to be completed in that specific order.)\n",
        "\n",
        "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
      ]
    },
    {
      "metadata": {
        "id": "1xzdhyTS_3F9",
        "colab_type": "code",
        "outputId": "d47f987b-7002-42b6-90f3-4e609e6e847a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\")\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText\n",
              "0          0                       is so sad for my APL frie...\n",
              "1          0                     I missed the New Moon trail...\n",
              "2          1                            omg its already 7:30 :O\n",
              "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
              "4          0           i think mi bf is cheating on me!!!   ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "f49LJGhKE_oJ",
        "colab_type": "code",
        "outputId": "de8415f6-6a09-4633-ff26-cdcb4d3e0ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\")\n",
        "df.head()\n",
        "\n",
        "def standardize_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
        "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
        "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
        "    return df\n",
        "  \n",
        "df = standardize_text(df, 'SentimentText')\n",
        "\n",
        "\n",
        "### Denoise functions, apply to text ### \n",
        "def encode_decode(text):\n",
        "    \"\"\"Utility function to clean text by decoding HTML text.\"\"\"\n",
        "    unescaped = html.unescape(text)\n",
        "    return unescaped\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "  \n",
        "\n",
        "def denoise_text(text):\n",
        "    text = encode_decode(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "df['SentimentText'] = df['SentimentText'].apply(denoise_text)\n",
        "\n",
        "\n",
        "### Normalizing functions, apply to tokenized lists ###\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words or string\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = (unicodedata.normalize('NFKD', word)\n",
        "                    .encode('ascii','ignore').decode('utf-8', 'ignore'))\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "  \n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words or string, also \n",
        "    lowercases every word.\n",
        "    \"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s\\_]|\\_', '', word)\n",
        "        new_word = new_word.lower()\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "  \n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words or string \n",
        "    with textual representation.\n",
        "    \"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words  \n",
        "  \n",
        "  \n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    #words = remove_stopwords(words)\n",
        "    return words\n",
        "  \n",
        "#def remove_stopwords(words):\n",
        "#    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "#    new_words = []\n",
        "#    for word in words:\n",
        "#        if word not in stopwords:\n",
        "#            new_words.append(word)\n",
        "#    return new_words\n",
        "  \n",
        "  \n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "df['SentimentText'] = df['SentimentText'].apply(tokenizer.tokenize)\n",
        "df['SentimentText'] = [normalize(token) for token in df['SentimentText']]\n",
        "\n",
        "\n",
        "cleaned_words = [word for tokens in df['SentimentText'] for word in tokens]\n",
        "cleanedVOCAB = sorted(list(set(cleaned_words)))\n",
        "display(\"%s words total, with a vocabulary size of %s\" % (len(cleaned_words), len(cleanedVOCAB)))\n",
        "\n",
        "display(df.head(20))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'1277018 words total, with a vocabulary size of 52722'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[is, so, sad, for, my, apl, friend]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[i, missed, the, new, moon, trailer]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[omg, its, already, seven, thirty, o]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[omgaga, im, sooo, im, gunna, cry, i, ve, been...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[i, think, mi, bf, is, cheating, on, me, tt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>[or, i, just, worry, too, much]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>[juuuuuuuuuuuuuuuuussssst, chillin]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>[sunny, again, work, tomorrow, tv, tonight]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>[handed, in, my, uniform, today, i, miss, you,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>[hmmmm, i, wonder, how, she, my, number]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>[i, must, think, about, positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>[thanks, to, all, the, haters, up, in, my, fac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>[this, weekend, has, sucked, so, far]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>[jb, isnt, showing, in, australia, any, more]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>[ok, thats, it, you, win]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>[this, is, the, way, i, feel, right, now]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>[awhhe, man, i, m, completely, useless, rt, no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>[feeling, strangely, fine, now, i, m, gonna, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>[huge, roll, of, thunder, just, now, so, scary]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>[i, just, cut, my, beard, off, it, s, only, be...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentiment                                      SentimentText\n",
              "0           0                [is, so, sad, for, my, apl, friend]\n",
              "1           0               [i, missed, the, new, moon, trailer]\n",
              "2           1              [omg, its, already, seven, thirty, o]\n",
              "3           0  [omgaga, im, sooo, im, gunna, cry, i, ve, been...\n",
              "4           0       [i, think, mi, bf, is, cheating, on, me, tt]\n",
              "5           0                    [or, i, just, worry, too, much]\n",
              "6           1                [juuuuuuuuuuuuuuuuussssst, chillin]\n",
              "7           0        [sunny, again, work, tomorrow, tv, tonight]\n",
              "8           1  [handed, in, my, uniform, today, i, miss, you,...\n",
              "9           1           [hmmmm, i, wonder, how, she, my, number]\n",
              "10          0                  [i, must, think, about, positive]\n",
              "11          1  [thanks, to, all, the, haters, up, in, my, fac...\n",
              "12          0              [this, weekend, has, sucked, so, far]\n",
              "13          0      [jb, isnt, showing, in, australia, any, more]\n",
              "14          0                          [ok, thats, it, you, win]\n",
              "15          0          [this, is, the, way, i, feel, right, now]\n",
              "16          0  [awhhe, man, i, m, completely, useless, rt, no...\n",
              "17          1  [feeling, strangely, fine, now, i, m, gonna, g...\n",
              "18          0    [huge, roll, of, thunder, just, now, so, scary]\n",
              "19          0  [i, just, cut, my, beard, off, it, s, only, be..."
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Q764vszGqiUh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How should TF-IDF scores be interpreted? How are they calculated?"
      ]
    },
    {
      "metadata": {
        "id": "e2Ji7BMhqs3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for **Term Frequency - Inverse Document Frequency**\n",
        "\n",
        "**Term Frequency** shows how frequently an expression (term, word) occurs in a document, with the insight being that the more frequently a term is used, the greater its importance.  It is calculated by taking the number of times a word appears in a document, divided by the total number of words in that document\n",
        "\n",
        "In sklearn TF-IDF Transformer it is caluclated by: ***tf(t, d)***\n",
        "\n",
        "It is well understood however the correlation between Term Frequency != Importance in all cases. \n",
        "Often we see terms that occur so often they necessitate removal via stopwords removal, as they are trivial but frequent. Hence frequency alone is not conclusive. \n",
        "\n",
        "**Inverse Document Frequency**  It is a method of determining the frequency of a word within a dataset of texts. Imagine it as a ratio of all existing texts/documents of an entire dataset and the number of texts that contain the defined keyword.  \n",
        "\n",
        "In sklearn TF-IDF Transformer it is caluclated by: ***idf(t) = log [ n / df(t) ] + 1***\n",
        "\n",
        "n = total_#_of_documents\n",
        "df(t) #_of_docs_containing_term;  the document frequency of t;  the number of documents in the document set that contain the term t. \n",
        "\n",
        "As the document Frequency grows this fraction (total/target) becomes smaller.\n",
        "\n",
        "**TF-IDF** or more accurately (*TF* x *IDF*) is where we multiply the Term Frequency by the Inverse Document Frequency.  We essentially weight the Term Frequency by the Inverse Document Frequency so that words that occur too frequently are penalized, and words that may be more infrequent but appear across multiple documents are boosted.  This better helps sharpen the signal of our input data and allows our models to operate with a clearer distinction between signal (important words) & noise (frequent but inconsequential words). \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3iUeBKtG_uEK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Document Classification\n",
        "\n",
        "1) Use Train_Test_Split to create train and test datasets.\n",
        "\n",
        "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
        "\n",
        " - Stretch goal: Use both of the methods that we talked about in class.\n",
        "\n",
        "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
        "\n",
        "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
        "\n",
        " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gj0uXB0ebD5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bag of Words w/ Count Vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "hHP6Lsq3bM59",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "seed = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TX8OEgUP_3ee",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Defining the Count Vectorizer function. \n",
        "def cv(data):\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    emb = count_vectorizer.fit_transform(data)\n",
        "    return emb, count_vectorizer\n",
        "\n",
        "# Here I define my corpus and lables. These are used throughout all the other models/processes.\n",
        "list_corpus = df[\"SentimentText\"].apply(lambda y: \" \".join(str(x) for x in y)).tolist()\n",
        "list_labels = df[\"Sentiment\"].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n",
        "                                                                                random_state=seed)\n",
        "\n",
        "X_train_counts, count_vectorizer = cv(X_train)\n",
        "X_test_counts = count_vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puQvju3WbClz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=40)\n",
        "clf.fit(X_train_counts, y_train)\n",
        "\n",
        "y_predicted_counts = clf.predict(X_test_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7YRLWgMziwE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3906114-2eee-4b5e-8ce1-9b79ee020408"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "def get_metrics(y_test, y_predicted):  \n",
        "    # true positives / (true positives+false positives)\n",
        "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
        "                                    average='weighted')             \n",
        "    # true positives / (true positives + false negatives)\n",
        "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
        "                              average='weighted')\n",
        "    \n",
        "    # harmonic mean of precision and recall\n",
        "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
        "    \n",
        "    # true positives + true negatives/ total\n",
        "    accuracy = accuracy_score(y_test, y_predicted)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
        "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 0.734, precision = 0.735, recall = 0.734, f1 = 0.735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1sAfftg-kUzQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "724407b9-2d83-47b5-dbd6-3abda1a785d6"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "MNB = MultinomialNB().fit(X_train_counts, y_train)\n",
        "\n",
        "train_predictions = MNB.predict(X_train_counts)\n",
        "test_predictions = MNB.predict(X_test_counts)\n",
        "\n",
        "accuracy, precision, recall, f1 = get_metrics(y_test, test_predictions)\n",
        "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 0.758, precision = 0.758, recall = 0.758, f1 = 0.758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "csIkYNQQjfyQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now BoW TF-IDF"
      ]
    },
    {
      "metadata": {
        "id": "WQyCqZvtjEDR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tfidf(data):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "    train = tfidf_vectorizer.fit_transform(data)\n",
        "\n",
        "    return train, tfidf_vectorizer\n",
        "\n",
        "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDoskDcnjDqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
        "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
        "clf_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "um6uo-_bjI6v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71e2d6a0-2b01-4705-9da3-671c80735ebd"
      },
      "cell_type": "code",
      "source": [
        "accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)\n",
        "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf, \n",
        "                                                                       recall_tfidf, f1_tfidf))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 0.743, precision = 0.744, recall = 0.743, f1 = 0.743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sorF95UO_uGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4 -  Word2Vec\n",
        "\n",
        "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
        "\n",
        "2) Display the 10 words that are most similar to the word \"twitter\""
      ]
    },
    {
      "metadata": {
        "id": "DYno4d4N-LHR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.word2vec import Word2Vec\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}